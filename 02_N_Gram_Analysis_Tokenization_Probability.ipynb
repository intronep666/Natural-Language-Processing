{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "_H71nmPHedIu"
      },
      "source": [
        "## Objective\n",
        "- Tokenize the given text using `nltk.word_tokenize`.\n",
        "- Remove punctuation tokens to clean the data.\n",
        "- Generate unigrams, bigrams, and trigrams using `nltk.ngrams`.\n",
        "- Count the frequency of each n-gram using `collections.Counter`.\n",
        "- Calculate the probability of each n-gram.\n"
      ],
      "id": "_H71nmPHedIu"
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "MMJDNGNeedIv"
      },
      "source": [
        "## Step-wise Explanation\n",
        "1. **Tokenization:** We start by using NLTK's `word_tokenize` function to split the input text into tokens. This includes words and punctuation marks as separate tokens.\n",
        "2. **Punctuation Removal:** Next, we filter out tokens that are not alphanumeric (i.e., remove punctuation). This gives a cleaner list of word tokens.\n",
        "3. **N-gram Generation:** We generate unigrams, bigrams, and trigrams from the filtered tokens using `nltk.ngrams`, which takes the list of tokens and n (1, 2, or 3) to create sequences of words.\n",
        "4. **Frequency Counting:** Using `collections.Counter`, we count how many times each n-gram occurs in the text. This provides a frequency distribution for unigrams, bigrams, and trigrams.\n",
        "5. **Probability Calculation:** Finally, we calculate the probability of each n-gram by dividing its frequency by the total number of n-grams of that type. This gives a relative frequency (probability) of each n-gram.\n"
      ],
      "id": "MMJDNGNeedIv"
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "3_0-M3aDedIv"
      },
      "source": [
        "## Code\n"
      ],
      "id": "3_0-M3aDedIv"
    },
    {
      "cell_type": "code",
      "execution_count": 1,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 0
        },
        "id": "4DtBt74SedIw",
        "outputId": "cef156fb-4e6b-4ff9-9c5d-d1830e6c9d69"
      },
      "source": [
        "import nltk\n",
        "from nltk.tokenize import word_tokenize\n",
        "from nltk.util import ngrams\n",
        "from collections import Counter\n",
        "import string\n",
        "import os\n",
        "\n",
        "# Set NLTK data path to a writable directory\n",
        "nltk.data.path.append('/tmp/nltk_data')\n",
        "if not os.path.exists('/tmp/nltk_data'):\n",
        "    os.makedirs('/tmp/nltk_data')\n",
        "\n",
        "\n",
        "# Download the punkt tokenizer (for tokenization)\n",
        "nltk.download('punkt')\n",
        "nltk.download('punkt_tab') # Attempt to download punkt_tab\n",
        "\n",
        "# Define a sample text\n",
        "text = \"NLP is amazing. It is widely used in AI applications, including speech recognition; people love analyzing text data!\"\n",
        "\n",
        "# 1. Tokenization\n",
        "tokens = word_tokenize(text)\n",
        "print(\"Tokens:\", tokens)\n",
        "\n",
        "# 2. Remove punctuation\n",
        "filtered_tokens = [token for token in tokens if token.isalnum()]\n",
        "print(\"Tokens after punctuation removal:\", filtered_tokens)\n",
        "\n",
        "# 3. Generate n-grams\n",
        "unigrams = list(ngrams(filtered_tokens, 1))\n",
        "bigrams = list(ngrams(filtered_tokens, 2))\n",
        "trigrams = list(ngrams(filtered_tokens, 3))\n",
        "print(\"Unigrams:\", unigrams)\n",
        "print(\"Bigrams:\", bigrams)\n",
        "print(\"Trigrams:\", trigrams)\n",
        "\n",
        "# 4. Frequency counts\n",
        "freq_uni = Counter(unigrams)\n",
        "freq_bi = Counter(bigrams)\n",
        "freq_tri = Counter(trigrams)\n",
        "print(\"Unigram frequencies:\", freq_uni)\n",
        "print(\"Bigram frequencies:\", freq_bi)\n",
        "print(\"Trigram frequencies:\", freq_tri)\n",
        "\n",
        "# 5. Probability calculations\n",
        "total_uni = sum(freq_uni.values())\n",
        "total_bi = sum(freq_bi.values())\n",
        "total_tri = sum(freq_tri.values())\n",
        "print(f\"Total unigrams: {total_uni}, Total bigrams: {total_bi}, Total trigrams: {total_tri}\")\n",
        "\n",
        "print(\"Unigram probabilities:\")\n",
        "for uni, count in freq_uni.items():\n",
        "    print(f\"{uni} -> {count} / {total_uni} = {count/total_uni}\")\n",
        "print(\"Bigram probabilities:\")\n",
        "for bi, count in freq_bi.items():\n",
        "    print(f\"{bi} -> {count} / {total_bi} = {count/total_bi}\")\n",
        "print(\"Trigram probabilities:\")\n",
        "for tri, count in freq_tri.items():\n",
        "    print(f\"{tri} -> {count} / {total_tri} = {count/total_tri}\")"
      ],
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "[nltk_data] Downloading package punkt to /root/nltk_data...\n",
            "[nltk_data]   Unzipping tokenizers/punkt.zip.\n",
            "[nltk_data] Downloading package punkt_tab to /root/nltk_data...\n",
            "[nltk_data]   Unzipping tokenizers/punkt_tab.zip.\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Tokens: ['NLP', 'is', 'amazing', '.', 'It', 'is', 'widely', 'used', 'in', 'AI', 'applications', ',', 'including', 'speech', 'recognition', ';', 'people', 'love', 'analyzing', 'text', 'data', '!']\n",
            "Tokens after punctuation removal: ['NLP', 'is', 'amazing', 'It', 'is', 'widely', 'used', 'in', 'AI', 'applications', 'including', 'speech', 'recognition', 'people', 'love', 'analyzing', 'text', 'data']\n",
            "Unigrams: [('NLP',), ('is',), ('amazing',), ('It',), ('is',), ('widely',), ('used',), ('in',), ('AI',), ('applications',), ('including',), ('speech',), ('recognition',), ('people',), ('love',), ('analyzing',), ('text',), ('data',)]\n",
            "Bigrams: [('NLP', 'is'), ('is', 'amazing'), ('amazing', 'It'), ('It', 'is'), ('is', 'widely'), ('widely', 'used'), ('used', 'in'), ('in', 'AI'), ('AI', 'applications'), ('applications', 'including'), ('including', 'speech'), ('speech', 'recognition'), ('recognition', 'people'), ('people', 'love'), ('love', 'analyzing'), ('analyzing', 'text'), ('text', 'data')]\n",
            "Trigrams: [('NLP', 'is', 'amazing'), ('is', 'amazing', 'It'), ('amazing', 'It', 'is'), ('It', 'is', 'widely'), ('is', 'widely', 'used'), ('widely', 'used', 'in'), ('used', 'in', 'AI'), ('in', 'AI', 'applications'), ('AI', 'applications', 'including'), ('applications', 'including', 'speech'), ('including', 'speech', 'recognition'), ('speech', 'recognition', 'people'), ('recognition', 'people', 'love'), ('people', 'love', 'analyzing'), ('love', 'analyzing', 'text'), ('analyzing', 'text', 'data')]\n",
            "Unigram frequencies: Counter({('is',): 2, ('NLP',): 1, ('amazing',): 1, ('It',): 1, ('widely',): 1, ('used',): 1, ('in',): 1, ('AI',): 1, ('applications',): 1, ('including',): 1, ('speech',): 1, ('recognition',): 1, ('people',): 1, ('love',): 1, ('analyzing',): 1, ('text',): 1, ('data',): 1})\n",
            "Bigram frequencies: Counter({('NLP', 'is'): 1, ('is', 'amazing'): 1, ('amazing', 'It'): 1, ('It', 'is'): 1, ('is', 'widely'): 1, ('widely', 'used'): 1, ('used', 'in'): 1, ('in', 'AI'): 1, ('AI', 'applications'): 1, ('applications', 'including'): 1, ('including', 'speech'): 1, ('speech', 'recognition'): 1, ('recognition', 'people'): 1, ('people', 'love'): 1, ('love', 'analyzing'): 1, ('analyzing', 'text'): 1, ('text', 'data'): 1})\n",
            "Trigram frequencies: Counter({('NLP', 'is', 'amazing'): 1, ('is', 'amazing', 'It'): 1, ('amazing', 'It', 'is'): 1, ('It', 'is', 'widely'): 1, ('is', 'widely', 'used'): 1, ('widely', 'used', 'in'): 1, ('used', 'in', 'AI'): 1, ('in', 'AI', 'applications'): 1, ('AI', 'applications', 'including'): 1, ('applications', 'including', 'speech'): 1, ('including', 'speech', 'recognition'): 1, ('speech', 'recognition', 'people'): 1, ('recognition', 'people', 'love'): 1, ('people', 'love', 'analyzing'): 1, ('love', 'analyzing', 'text'): 1, ('analyzing', 'text', 'data'): 1})\n",
            "Total unigrams: 18, Total bigrams: 17, Total trigrams: 16\n",
            "Unigram probabilities:\n",
            "('NLP',) -> 1 / 18 = 0.05555555555555555\n",
            "('is',) -> 2 / 18 = 0.1111111111111111\n",
            "('amazing',) -> 1 / 18 = 0.05555555555555555\n",
            "('It',) -> 1 / 18 = 0.05555555555555555\n",
            "('widely',) -> 1 / 18 = 0.05555555555555555\n",
            "('used',) -> 1 / 18 = 0.05555555555555555\n",
            "('in',) -> 1 / 18 = 0.05555555555555555\n",
            "('AI',) -> 1 / 18 = 0.05555555555555555\n",
            "('applications',) -> 1 / 18 = 0.05555555555555555\n",
            "('including',) -> 1 / 18 = 0.05555555555555555\n",
            "('speech',) -> 1 / 18 = 0.05555555555555555\n",
            "('recognition',) -> 1 / 18 = 0.05555555555555555\n",
            "('people',) -> 1 / 18 = 0.05555555555555555\n",
            "('love',) -> 1 / 18 = 0.05555555555555555\n",
            "('analyzing',) -> 1 / 18 = 0.05555555555555555\n",
            "('text',) -> 1 / 18 = 0.05555555555555555\n",
            "('data',) -> 1 / 18 = 0.05555555555555555\n",
            "Bigram probabilities:\n",
            "('NLP', 'is') -> 1 / 17 = 0.058823529411764705\n",
            "('is', 'amazing') -> 1 / 17 = 0.058823529411764705\n",
            "('amazing', 'It') -> 1 / 17 = 0.058823529411764705\n",
            "('It', 'is') -> 1 / 17 = 0.058823529411764705\n",
            "('is', 'widely') -> 1 / 17 = 0.058823529411764705\n",
            "('widely', 'used') -> 1 / 17 = 0.058823529411764705\n",
            "('used', 'in') -> 1 / 17 = 0.058823529411764705\n",
            "('in', 'AI') -> 1 / 17 = 0.058823529411764705\n",
            "('AI', 'applications') -> 1 / 17 = 0.058823529411764705\n",
            "('applications', 'including') -> 1 / 17 = 0.058823529411764705\n",
            "('including', 'speech') -> 1 / 17 = 0.058823529411764705\n",
            "('speech', 'recognition') -> 1 / 17 = 0.058823529411764705\n",
            "('recognition', 'people') -> 1 / 17 = 0.058823529411764705\n",
            "('people', 'love') -> 1 / 17 = 0.058823529411764705\n",
            "('love', 'analyzing') -> 1 / 17 = 0.058823529411764705\n",
            "('analyzing', 'text') -> 1 / 17 = 0.058823529411764705\n",
            "('text', 'data') -> 1 / 17 = 0.058823529411764705\n",
            "Trigram probabilities:\n",
            "('NLP', 'is', 'amazing') -> 1 / 16 = 0.0625\n",
            "('is', 'amazing', 'It') -> 1 / 16 = 0.0625\n",
            "('amazing', 'It', 'is') -> 1 / 16 = 0.0625\n",
            "('It', 'is', 'widely') -> 1 / 16 = 0.0625\n",
            "('is', 'widely', 'used') -> 1 / 16 = 0.0625\n",
            "('widely', 'used', 'in') -> 1 / 16 = 0.0625\n",
            "('used', 'in', 'AI') -> 1 / 16 = 0.0625\n",
            "('in', 'AI', 'applications') -> 1 / 16 = 0.0625\n",
            "('AI', 'applications', 'including') -> 1 / 16 = 0.0625\n",
            "('applications', 'including', 'speech') -> 1 / 16 = 0.0625\n",
            "('including', 'speech', 'recognition') -> 1 / 16 = 0.0625\n",
            "('speech', 'recognition', 'people') -> 1 / 16 = 0.0625\n",
            "('recognition', 'people', 'love') -> 1 / 16 = 0.0625\n",
            "('people', 'love', 'analyzing') -> 1 / 16 = 0.0625\n",
            "('love', 'analyzing', 'text') -> 1 / 16 = 0.0625\n",
            "('analyzing', 'text', 'data') -> 1 / 16 = 0.0625\n"
          ]
        }
      ],
      "id": "4DtBt74SedIw"
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "n7fDr545edIx"
      },
      "source": [
        "## Output\n",
        "```\n",
        "Tokens: ['NLP', 'is', 'amazing', '.', 'It', 'is', 'widely', 'used', 'in', 'AI', 'applications', ',', 'including', 'speech', 'recognition', ';', 'people', 'love', 'analyzing', 'text', 'data', '!']\n",
        "Tokens after punctuation removal: ['NLP', 'is', 'amazing', 'It', 'is', 'widely', 'used', 'in', 'AI', 'applications', 'including', 'speech', 'recognition', 'people', 'love', 'analyzing', 'text', 'data']\n",
        "Unigrams: [('NLP',), ('is',), ('amazing',), ('It',), ('is',), ('widely',), ('used',), ('in',), ('AI',), ('applications',), ('including',), ('speech',), ('recognition',), ('people',), ('love',), ('analyzing',), ('text',), ('data',)]\n",
        "Bigrams: [('NLP', 'is'), ('is', 'amazing'), ('amazing', 'It'), ('It', 'is'), ('is', 'widely'), ('widely', 'used'), ('used', 'in'), ('in', 'AI'), ('AI', 'applications'), ('applications', 'including'), ('including', 'speech'), ('speech', 'recognition'), ('recognition', 'people'), ('people', 'love'), ('love', 'analyzing'), ('analyzing', 'text'), ('text', 'data')]\n",
        "Trigrams: [('NLP', 'is', 'amazing'), ('is', 'amazing', 'It'), ('amazing', 'It', 'is'), ('It', 'is', 'widely'), ('is', 'widely', 'used'), ('widely', 'used', 'in'), ('used', 'in', 'AI'), ('in', 'AI', 'applications'), ('AI', 'applications', 'including'), ('applications', 'including', 'speech'), ('including', 'speech', 'recognition'), ('speech', 'recognition', 'people'), ('recognition', 'people', 'love'), ('people', 'love', 'analyzing'), ('love', 'analyzing', 'text'), ('analyzing', 'text', 'data')]\n",
        "Unigram frequencies: Counter({('is',): 2, ('NLP',): 1, ('amazing',): 1, ('It',): 1, ('widely',): 1, ('used',): 1, ('in',): 1, ('AI',): 1, ('applications',): 1, ('including',): 1, ('speech',): 1, ('recognition',): 1, ('people',): 1, ('love',): 1, ('analyzing',): 1, ('text',): 1, ('data',): 1})\n",
        "Bigram frequencies: Counter({('NLP', 'is'): 1, ('is', 'amazing'): 1, ('amazing', 'It'): 1, ('It', 'is'): 1, ('is', 'widely'): 1, ('widely', 'used'): 1, ('used', 'in'): 1, ('in', 'AI'): 1, ('AI', 'applications'): 1, ('applications', 'including'): 1, ('including', 'speech'): 1, ('speech', 'recognition'): 1, ('recognition', 'people'): 1, ('people', 'love'): 1, ('love', 'analyzing'): 1, ('analyzing', 'text'): 1, ('text', 'data'): 1})\n",
        "Trigram frequencies: Counter({('NLP', 'is', 'amazing'): 1, ('is', 'amazing', 'It'): 1, ('amazing', 'It', 'is'): 1, ('It', 'is', 'widely'): 1, ('is', 'widely', 'used'): 1, ('widely', 'used', 'in'): 1, ('used', 'in', 'AI'): 1, ('in', 'AI', 'applications'): 1, ('AI', 'applications', 'including'): 1, ('applications', 'including', 'speech'): 1, ('including', 'speech', 'recognition'): 1, ('speech', 'recognition', 'people'): 1, ('recognition', 'people', 'love'): 1, ('people', 'love', 'analyzing'): 1, ('love', 'analyzing', 'text'): 1, ('analyzing', 'text', 'data'): 1})\n",
        "Total unigrams: 18, Total bigrams: 17, Total trigrams: 16\n",
        "Unigram probabilities:\n",
        "('NLP',) -> 1 / 18 = 0.05555555555555555\n",
        "('is',) -> 2 / 18 = 0.1111111111111111\n",
        "('amazing',) -> 1 / 18 = 0.05555555555555555\n",
        "('It',) -> 1 / 18 = 0.05555555555555555\n",
        "('widely',) -> 1 / 18 = 0.05555555555555555\n",
        "('used',) -> 1 / 18 = 0.05555555555555555\n",
        "('in',) -> 1 / 18 = 0.05555555555555555\n",
        "('AI',) -> 1 / 18 = 0.05555555555555555\n",
        "('applications',) -> 1 / 18 = 0.05555555555555555\n",
        "('including',) -> 1 / 18 = 0.05555555555555555\n",
        "('speech',) -> 1 / 18 = 0.05555555555555555\n",
        "('recognition',) -> 1 / 18 = 0.05555555555555555\n",
        "('people',) -> 1 / 18 = 0.05555555555555555\n",
        "('love',) -> 1 / 18 = 0.05555555555555555\n",
        "('analyzing',) -> 1 / 18 = 0.05555555555555555\n",
        "('text',) -> 1 / 18 = 0.05555555555555555\n",
        "('data',) -> 1 / 18 = 0.05555555555555555\n",
        "Bigram probabilities:\n",
        "('NLP', 'is') -> 1 / 17 = 0.058823529411764705\n",
        "('is', 'amazing') -> 1 / 17 = 0.058823529411764705\n",
        "('amazing', 'It') -> 1 / 17 = 0.058823529411764705\n",
        "('It', 'is') -> 1 / 17 = 0.058823529411764705\n",
        "('is', 'widely') -> 1 / 17 = 0.058823529411764705\n",
        "('widely', 'used') -> 1 / 17 = 0.058823529411764705\n",
        "('used', 'in') -> 1 / 17 = 0.058823529411764705\n",
        "('in', 'AI') -> 1 / 17 = 0.058823529411764705\n",
        "('AI', 'applications') -> 1 / 17 = 0.058823529411764705\n",
        "('applications', 'including') -> 1 / 17 = 0.058823529411764705\n",
        "('including', 'speech') -> 1 / 17 = 0.058823529411764705\n",
        "('speech', 'recognition') -> 1 / 17 = 0.058823529411764705\n",
        "('recognition', 'people') -> 1 / 17 = 0.058823529411764705\n",
        "('people', 'love') -> 1 / 17 = 0.058823529411764705\n",
        "('love', 'analyzing') -> 1 / 17 = 0.058823529411764705\n",
        "('analyzing', 'text') -> 1 / 17 = 0.058823529411764705\n",
        "('text', 'data') -> 1 / 17 = 0.058823529411764705\n",
        "Trigram probabilities:\n",
        "('NLP', 'is', 'amazing') -> 1 / 16 = 0.0625\n",
        "('is', 'amazing', 'It') -> 1 / 16 = 0.0625\n",
        "('amazing', 'It', 'is') -> 1 / 16 = 0.0625\n",
        "('It', 'is', 'widely') -> 1 / 16 = 0.0625\n",
        "('is', 'widely', 'used') -> 1 / 16 = 0.0625\n",
        "('widely', 'used', 'in') -> 1 / 16 = 0.0625\n",
        "('used', 'in', 'AI') -> 1 / 16 = 0.0625\n",
        "('in', 'AI', 'applications') -> 1 / 16 = 0.0625\n",
        "('AI', 'applications', 'including') -> 1 / 16 = 0.0625\n",
        "('applications', 'including', 'speech') -> 1 / 16 = 0.0625\n",
        "('including', 'speech', 'recognition') -> 1 / 16 = 0.0625\n",
        "('speech', 'recognition', 'people') -> 1 / 16 = 0.0625\n",
        "('recognition', 'people', 'love') -> 1 / 16 = 0.0625\n",
        "('people', 'love', 'analyzing') -> 1 / 16 = 0.0625\n",
        "('love', 'analyzing', 'text') -> 1 / 16 = 0.0625\n",
        "('analyzing', 'text', 'data') -> 1 / 16 = 0.0625\n",
        "```"
      ],
      "id": "n7fDr545edIx"
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "oVhv5id9edIy"
      },
      "source": [
        "## Conclusion\n",
        "In this practical, we successfully tokenized the input text, removed punctuation, and generated unigrams, bigrams, and trigrams. By counting the frequency of each n-gram and computing their probabilities, we obtained the relative likelihood of each sequence in the text. This simple NLP pipeline demonstrates how we can process raw text data and extract useful statistical information about word sequences. Such frequency and probability calculations form a fundamental basis for many NLP tasks, including language modeling and text analysis.\n"
      ],
      "id": "oVhv5id9edIy"
    }
  ],
  "metadata": {
    "kernelspec": {
      "display_name": "Python 3",
      "language": "python",
      "name": "python3"
    },
    "language_info": {
      "name": "python",
      "version": "3.x"
    },
    "colab": {
      "provenance": []
    }
  },
  "nbformat": 4,
  "nbformat_minor": 5
}